import os
import tempfile
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from youtube_transcript_api import YouTubeTranscriptApi
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ ì¶”ê°€
def normalize_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤."""
    if text is None:
        return ""
    # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ì¶”ê°€ ì •ê·œí™” ì ìš©
    return text.lower().strip()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide", page_title="Multi-Document ChatBot")

# ë©”ì¸ íƒ€ì´í‹€
st.title("ğŸ¤– ë‹¤ì¤‘ ë¬¸ì„œ í†µí•© RAG Chatbot")
st.write("ì›¹í˜ì´ì§€, PDF, YouTube ì˜ìƒì„ ë™ì‹œì— ì„ íƒí•˜ì—¬ í†µí•© ê²€ìƒ‰í•´ë³´ì„¸ìš”!")

# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def process_query(prompt):
    if not prompt.strip():
        return  # ë¹ˆ ì…ë ¥ì€ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        
    if not st.session_state.vector_store:
        st.error("ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”!")
        return
        
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    
    # ì‚¬ìš©ì ì…ë ¥ ì •ê·œí™” (ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ëŒ€ì†Œë¬¸ìë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•¨)
    normalized_prompt = normalize_text(prompt)
    
    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.spinner("ìƒê° ì¤‘..."):
        llm = ChatOpenAI(model_name=selected_model, temperature=0)
        
        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì¿¼ë¦¬ êµ¬ì„±
        # ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆê³  ì—¬ëŸ¬ ì¡°í•©ì„ ì‹œë„
        variations = [
            normalized_prompt,  # ì†Œë¬¸ì ë²„ì „
            prompt.upper(),     # ëŒ€ë¬¸ì ë²„ì „
            prompt,             # ì›ë³¸ ê·¸ëŒ€ë¡œ
        ]
        
        # ì¤‘ë³µ ì œê±°
        variations = list(set(variations))
        
        all_docs = []
        for query_var in variations:
            # ê° ë³€í˜•ì— ëŒ€í•´ ê²€ìƒ‰ ì‹¤í–‰
            docs = st.session_state.vector_store.similarity_search(
                query_var, 
                k=2,  # ê° ë³€í˜•ë§ˆë‹¤ ìƒìœ„ 2ê°œì”© ê°€ì ¸ì˜´
                fetch_k=5  # ì´ˆê¸° í›„ë³´ëŠ” ë” ë§ì´
            )
            all_docs.extend(docs)
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ë¬¸ì„œê°€ ì—¬ëŸ¬ ì¿¼ë¦¬ ë³€í˜•ì—ì„œ ê²€ìƒ‰ë  ìˆ˜ ìˆìŒ)
        unique_docs = []
        doc_contents = set()
        for doc in all_docs:
            if doc.page_content not in doc_contents:
                unique_docs.append(doc)
                doc_contents.add(doc.page_content)
        
        # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        top_docs = unique_docs[:3] if len(unique_docs) > 3 else unique_docs
        
        # QA ì²´ì¸ ì‹¤í–‰
        qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=st.session_state.vector_store.as_retriever(),
            return_source_documents=True
        )
        
        # ì‹¤ì œ QA ìˆ˜í–‰
        result = qa_chain.invoke({
            "query": normalized_prompt,
            "context": "\n\n".join([doc.page_content for doc in top_docs])
        })
        response = result["result"]
        response = result["result"]
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (ìœ íš¨í•œ ì‘ë‹µì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if "source_documents" in result and response and not response.startswith("I don't have information"):
            sources = []
            for doc in result["source_documents"][:3]:  # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ í‘œì‹œ
                source_type = doc.metadata.get("source_type", "unknown")
                source = doc.metadata.get("source", "unknown source")
                sources.append(f"- {source_type}: {source}")
            
            if sources:
                response += "\n\n**ì°¸ê³  ì¶œì²˜:**\n" + "\n".join(set(sources))
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "assistant", "content": response})

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def load_pdf(uploaded_file):
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
    with open(temp_filepath, "wb") as f:
        f.write(uploaded_file.getvalue())
    loader = PyPDFLoader(temp_filepath)
    documents = loader.load_and_split()
    # í…ìŠ¤íŠ¸ ì •ê·œí™” ì ìš©
    for doc in documents:
        doc.page_content = normalize_text(doc.page_content)
        # ì†ŒìŠ¤ íƒ€ì… ë©”íƒ€ë°ì´í„° ì¶”ê°€
        doc.metadata["source_type"] = "pdf"
        # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ metadataì— ì €ì¥í•˜ì—¬ í•„ìš”ì‹œ ì°¸ì¡°
        doc.metadata["original_text"] = doc.page_content
    return documents

def load_webpage(url):
    loader = WebBaseLoader(url)
    documents = loader.load()
    # í…ìŠ¤íŠ¸ ì •ê·œí™” ì ìš©
    for doc in documents:
        doc.page_content = normalize_text(doc.page_content)
        # ì†ŒìŠ¤ íƒ€ì… ë©”íƒ€ë°ì´í„° ì¶”ê°€
        doc.metadata["source_type"] = "web"
    return documents

def load_youtube(youtube_url):
    try:
        # URLì—ì„œ video_id ì¶”ì¶œ
        if "youtube.com/watch?v=" in youtube_url:
            video_id = youtube_url.split("youtube.com/watch?v=")[1].split("&")[0]
        elif "youtu.be/" in youtube_url:
            video_id = youtube_url.split("youtu.be/")[1].split("?")[0]
        else:
            st.error(f"ì˜¬ë°”ë¥¸ YouTube URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {youtube_url}")
            return []
        
        # ìë§‰ ê°€ì ¸ì˜¤ê¸°
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
            
            # ìë§‰ í…ìŠ¤íŠ¸ ê²°í•©
            full_text = ' '.join([entry['text'] for entry in transcript])
            
            # í…ìŠ¤íŠ¸ ì •ê·œí™” ì ìš©
            normalized_text = normalize_text(full_text)
            
            # Document ê°ì²´ ìƒì„±
            doc = Document(
                page_content=normalized_text,
                metadata={"source": youtube_url, "source_type": "youtube"}
            )
            
            return [doc]
            
        except Exception as e:
            st.error(f"ìë§‰ ì²˜ë¦¬ ì˜¤ë¥˜ ({youtube_url}): {str(e)}")
            return []
            
    except Exception as e:
        st.error(f"YouTube URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({youtube_url}): {str(e)}")
        return []

def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
        is_separator_regex=False,
    )
    return text_splitter.split_documents(documents)

def create_embeddings(texts, selected_embedding):
    # OpenAI ì„ë² ë”© ìƒì„±ì‹œ ì„¤ì •
    embeddings = OpenAIEmbeddings(
        model=selected_embedding,
        openai_api_key=os.environ["OPENAI_API_KEY"],
        # ì„ë² ë”© ì „ì— í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ì •ê·œí™”í•˜ì—¬ ëŒ€ì†Œë¬¸ì ë¬¸ì œ í•´ê²°
        embedding_ctx_length=8191,  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì„¤ì •
    )
    
    return embeddings

def create_vector_db(texts, embeddings):
    # í•­ìƒ FAISSë§Œ ì‚¬ìš©í•˜ê³ , ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # ê²€ìƒ‰ ì‹œ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë‚®ì¶° ë” ë§ì€ ê²°ê³¼ë¥¼ í¬í•¨í•˜ë„ë¡ ì„¤ì •
    # ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ì†Œë¬¸ì ì°¨ì´ê°€ ìˆì–´ë„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§
    vectorstore.similarity_search_with_score_threshold = 0.3
    
    return vectorstore

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    
    # OpenAI API Key ì…ë ¥
    api_key = st.text_input("OpenAI API Key", type="password")
    
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
    else:
        st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    
    # ëª¨ë¸ ì„ íƒ
    model_options = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
    selected_model = st.selectbox("LLM ëª¨ë¸ ì„ íƒ", model_options)
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    embedding_options = ["text-embedding-ada-002", "text-embedding-3-small", "text-embedding-3-large"]
    selected_embedding = st.selectbox("ì„ë² ë”© ëª¨ë¸ ì„ íƒ", embedding_options)
    
    # FAISS ì‚¬ìš© ì •ë³´ (ê³ ì •)
    st.info("ë²¡í„° DB: FAISS")
    
    st.markdown("---")
    
    # ë¬¸ì„œ ì†ŒìŠ¤ ì„ íƒ (ì²´í¬ë°•ìŠ¤ ì‚¬ìš©)
    st.header("ë¬¸ì„œ ì†ŒìŠ¤ ì„ íƒ")
    use_web = st.checkbox("ì›¹í˜ì´ì§€", value=False)
    use_pdf = st.checkbox("PDF íŒŒì¼", value=False)
    use_youtube = st.checkbox("YouTube ì˜ìƒ", value=False)
    
    # ë¬¸ì„œ ì†ŒìŠ¤ì— ë”°ë¥¸ ì…ë ¥ í•„ë“œë“¤
    all_sources = {}
    
    # ì›¹í˜ì´ì§€ URL ì…ë ¥
    if use_web:
        st.subheader("ì›¹í˜ì´ì§€ URL")
        web_urls = []
        num_web_inputs = st.number_input("ì›¹í˜ì´ì§€ URL ê°œìˆ˜", min_value=1, max_value=5, value=1)
        
        for i in range(num_web_inputs):
            web_url = st.text_input(f"ì›¹í˜ì´ì§€ URL {i+1}", key=f"web_{i}")
            if web_url:
                web_urls.append(web_url)
        
        all_sources["web"] = web_urls
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    if use_pdf:
        st.subheader("PDF íŒŒì¼")
        uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)
        all_sources["pdf"] = uploaded_files if uploaded_files else []
    
    # YouTube ë§í¬ ì…ë ¥
    if use_youtube:
        st.subheader("YouTube ì˜ìƒ ë§í¬")
        youtube_urls = []
        num_youtube_inputs = st.number_input("YouTube ë§í¬ ê°œìˆ˜", min_value=1, max_value=5, value=1)
        
        for i in range(num_youtube_inputs):
            youtube_url = st.text_input(f"YouTube ë§í¬ {i+1}", key=f"youtube_{i}")
            if youtube_url:
                youtube_urls.append(youtube_url)
        
        all_sources["youtube"] = youtube_urls
    
    # ë¬¸ì„œ ë¡œë“œ ë²„íŠ¼
    load_doc_button = st.button("ì„ íƒí•œ ëª¨ë“  ì†ŒìŠ¤ ë¡œë“œí•˜ê¸°")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "current_sources" not in st.session_state:
    st.session_state.current_sources = None

# UI ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False

# ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ - ì²˜ë¦¬ ìƒíƒœì— ë”°ë¼ ë ˆì´ì•„ì›ƒ ë³€ê²½
if not st.session_state.processing_complete:
    # ë¬¸ì„œ ì²˜ë¦¬ ì „/ì¤‘ ë ˆì´ì•„ì›ƒ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
    st.header("ë¬¸ì„œ ì²˜ë¦¬ ê³¼ì •")
    
    # ë¬¸ì„œ ë¡œë“œ í”„ë¡œì„¸ìŠ¤
    if load_doc_button:
        if not (use_web or use_pdf or use_youtube):
            st.error("ìµœì†Œí•œ í•˜ë‚˜ ì´ìƒì˜ ë¬¸ì„œ ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner("ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                all_documents = []
                source_summaries = []
                
                # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
                sources_info = {}
                
                # 1. ëª¨ë“  ë¬¸ì„œ ì†ŒìŠ¤ ë¡œë“œ
                documents_status = st.empty()
                documents_status.info("1. ë¬¸ì„œ ì†ŒìŠ¤ ë¡œë“œ ì¤‘...")
                
                # ì›¹í˜ì´ì§€ ì²˜ë¦¬
                if use_web and all_sources["web"]:
                    for i, url in enumerate(all_sources["web"]):
                        if url:
                            with st.spinner(f"ì›¹í˜ì´ì§€ ë¡œë“œ ì¤‘ ({i+1}/{len(all_sources['web'])}): {url}"):
                                web_docs = load_webpage(url)
                                if web_docs:
                                    all_documents.extend(web_docs)
                                    source_summaries.append(f"ì›¹: {url}")
                                    if "web" not in sources_info:
                                        sources_info["web"] = []
                                    sources_info["web"].append(url)
                
                # PDF ì²˜ë¦¬
                if use_pdf and all_sources["pdf"]:
                    for i, file in enumerate(all_sources["pdf"]):
                        with st.spinner(f"PDF ë¡œë“œ ì¤‘ ({i+1}/{len(all_sources['pdf'])}): {file.name}"):
                            pdf_docs = load_pdf(file)
                            if pdf_docs:
                                all_documents.extend(pdf_docs)
                                source_summaries.append(f"PDF: {file.name}")
                                if "pdf" not in sources_info:
                                    sources_info["pdf"] = []
                                sources_info["pdf"].append(file.name)
                
                # YouTube ì²˜ë¦¬
                if use_youtube and all_sources["youtube"]:
                    for i, url in enumerate(all_sources["youtube"]):
                        if url:
                            with st.spinner(f"YouTube ë¡œë“œ ì¤‘ ({i+1}/{len(all_sources['youtube'])}): {url}"):
                                youtube_docs = load_youtube(url)
                                if youtube_docs:
                                    all_documents.extend(youtube_docs)
                                    source_summaries.append(f"YouTube: {url}")
                                    if "youtube" not in sources_info:
                                        sources_info["youtube"] = []
                                    sources_info["youtube"].append(url)
                
                # ë¬¸ì„œ ë¡œë“œ ê²°ê³¼ í™•ì¸
                if all_documents:
                    documents_status.success(f"1. ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(all_documents)} ê°œì˜ ë¬¸ì„œ ì„¸ê·¸ë¨¼íŠ¸")
                    
                    # 2. ì²­í‚¹
                    chunking_status = st.empty()
                    chunking_status.info("2. ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
                    texts = split_documents(all_documents)
                    chunking_status.success(f"2. ì²­í‚¹ ì™„ë£Œ: {len(texts)} ê°œì˜ ì²­í¬ ìƒì„±ë¨")
                    
                    # 3. ì„ë² ë”©
                    embedding_status = st.empty()
                    embedding_status.info("3. ì„ë² ë”© ë²¡í„°í™” ì¤‘...")
                    embeddings = create_embeddings(texts, selected_embedding)
                    embedding_status.success("3. ì„ë² ë”© ë²¡í„°í™” ì™„ë£Œ")
                    
                    # 4. ë²¡í„° DB ì €ì¥
                    vectordb_status = st.empty()
                    vectordb_status.info("4. FAISS ë²¡í„° DBì— ì €ì¥ ì¤‘...")
                    vector_store = create_vector_db(texts, embeddings)
                    vectordb_status.success("4. FAISS ë²¡í„° DB ì €ì¥ ì™„ë£Œ")
                    
                    # ì„¸ì…˜ ìƒíƒœì— ë²¡í„° ìŠ¤í† ì–´ì™€ ì†ŒìŠ¤ ì •ë³´ ì €ì¥
                    st.session_state.vector_store = vector_store
                    st.session_state.current_sources = source_summaries
                    
                    # 5. LLM ì¤€ë¹„ ì™„ë£Œ
                    llm_status = st.empty()
                    llm_status.success("5. ì§ˆë¬¸ ì¤€ë¹„ ì™„ë£Œ!")
                    
                    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” (ìƒˆ ë¬¸ì„œ ì¡°í•©ì´ë¯€ë¡œ)
                    st.session_state.chat_history = []
                    
                    # ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
                    st.session_state.processing_complete = True
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì±„íŒ… UIë¡œ ì „í™˜
                    st.rerun()
                else:
                    documents_status.error("ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ì„ í™•ì¸í•˜ì„¸ìš”.")
else:
    # ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ í›„ ì±„íŒ… UI ë ˆì´ì•„ì›ƒ
    # 2ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    chat_col1, chat_col2 = st.columns([1, 3])
    
    with chat_col1:
        st.header("ë¡œë“œëœ ì†ŒìŠ¤")
        # ë¡œë“œëœ ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
        for source in st.session_state.current_sources:
            st.info(source)
            
        # ë‹¤ì‹œ ë¬¸ì„œ ë¡œë“œí•˜ê¸° ë²„íŠ¼
        if st.button("ìƒˆ ë¬¸ì„œ ë¡œë“œí•˜ê¸°"):
            st.session_state.processing_complete = False
            st.rerun()
            
        # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            st.rerun()
        
        # ì±„íŒ… ì…ë ¥ì°½ (ì™¼ìª½ ì»¬ëŸ¼)
        st.subheader("ì§ˆë¬¸ ì…ë ¥")

        # Streamlit ë°©ì‹ìœ¼ë¡œ Enter í‚¤ ì œì¶œ ì²˜ë¦¬
        # text_area ëŒ€ì‹  formì„ ì‚¬ìš©í•˜ì—¬ Enter í‚¤ ì²˜ë¦¬
        with st.form(key="query_form", clear_on_submit=True):
            user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="user_query")
            submit_button = st.form_submit_button("ì „ì†¡")
            
        # í¼ ì œì¶œ ì‹œ ì§ˆë¬¸ ì²˜ë¦¬
        if submit_button and user_input:
            process_query(user_input)
            # ì±„íŒ… ê¸°ë¡ ì—…ë°ì´íŠ¸ í›„ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
            st.rerun()
    
    with chat_col2:
        st.header("ì±—ë´‡ê³¼ì˜ ëŒ€í™”")
        
        # ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.chat_history:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])